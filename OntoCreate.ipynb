{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5a1e8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "13908\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full import models\n",
    "from ipynb.fs.full import data as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "966c1524",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-21 20:11:23.987841: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-21 20:11:25.465944: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/soumen/anaconda3/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-06-21 20:11:25.466146: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/soumen/anaconda3/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-06-21 20:11:25.466165: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import torch\n",
    "from owlready2 import *\n",
    "import pandas as pd\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "995459a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * WARNING: DataProperty http://www.semanticweb.org/soumen/ontologies/2023/4/mudra-ontology#hasCoordinate belongs to more than one entity types: [owl.ObjectProperty, owl.DatatypeProperty]; I'm trying to fix it...\n"
     ]
    }
   ],
   "source": [
    "#Global variable declaration\n",
    "root_dir = 'Mudra Dataset/Single hand'\n",
    "onto_filepath = 'mudraOntology.owl'\n",
    "onto = get_ontology(onto_filepath).load()\n",
    "fingerDict = {1:\"Thumb\", 5:\"Index\", 9:\"Middle\", 13:\"Ring\", 17:\"Little\"}\n",
    "annotationPath = \"Annotation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea4ecd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d86af4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class handTracker():\n",
    "    def __init__(self, mode=False, maxHands=2, detectionCon=0.5, modelComplexity=1, trackCon=0.5):\n",
    "        self.mode = mode\n",
    "        self.maxHands = maxHands\n",
    "        self.detectionCon = detectionCon\n",
    "        self.modelComplex = modelComplexity\n",
    "        self.trackCon = trackCon\n",
    "        self.mpHands = mp.solutions.hands\n",
    "        self.hands = self.mpHands.Hands(self.mpHands.Hands(self.mode, self.maxHands, self.modelComplex, self.detectionCon, self.trackCon))\n",
    "        self.mpDraw = mp.solutions.drawing_utils\n",
    "    \n",
    "    def handsFinder(self, image, draw=True):\n",
    "        imageRGB = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        self.results = self.hands.process(imageRGB)\n",
    "\n",
    "        #if self.results.multi_hand_landmarks:\n",
    "            #for handLms in self.results.multi_hand_landmarks:\n",
    "                #print(handLms)\n",
    "        return image\n",
    "    \n",
    "    def positionFinder(self, image, handNo=0, draw=True):\n",
    "        lmlist=[]\n",
    "        if self.results.multi_hand_landmarks:\n",
    "            Hand = self.results.multi_hand_landmarks[handNo]\n",
    "\n",
    "            for id, lm in enumerate(Hand.landmark):\n",
    "                h,w,c = image.shape\n",
    "                cx, cy = int(lm.x*w), int(lm.y*h)\n",
    "                lmlist.append([id,cx,cy])\n",
    "        return lmlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a758841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "if os.path.isdir(root_dir):\n",
    "    for dirs in os.listdir(root_dir):\n",
    "        full_path = os.path.join(root_dir, dirs)\n",
    "        csv_path = os.path.join(full_path, 'Annotation.csv')\n",
    "        csv_file = pd.read_csv(csv_path)\n",
    "        for i, row in csv_file.iterrows():\n",
    "            img_path = os.path.join(full_path, row[0])\n",
    "            ontoFileName = dirs + \"_\" + row[0].split(\".\")[0]\n",
    "            #print(ontoFileName)\n",
    "            image = cv2.imread(img_path)\n",
    "            data[ontoFileName] = image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d7ae9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_joint_angle(data, a, b, c, d, ontoFileName):\n",
    "    jointAngle = []\n",
    "    dr1 = [data[a][0] - data[0][0], data[a][1] - data[0][1], data[a][2] - data[0][2]]\n",
    "    dr2 = [data[b][0] - data[a][0], data[b][1] - data[a][1], data[b][2] - data[a][2]]\n",
    "    dr3 = [data[c][0] - data[b][0], data[c][1] - data[b][1], data[c][2] - data[b][2]]\n",
    "    dr4 = [data[d][0] - data[c][0], data[d][1] - data[c][1], data[d][2] - data[c][2]]\n",
    "    \n",
    "    m1 = math.sqrt(dr1[0] * dr1[0] + dr1[1] * dr1[1] + dr1[2] * dr1[2])\n",
    "    m2 = math.sqrt(dr2[0] * dr2[0] + dr2[1] * dr2[1] + dr2[2] * dr2[2])\n",
    "    DIPAngle = math.degrees(math.acos((dr1[0] * dr2[0] + dr1[1] * dr2[1] + dr1[2] * dr2[2])/(m1*m2)))\n",
    "    \n",
    "    m1 = m2\n",
    "    m2 = math.sqrt(dr3[0] * dr3[0] + dr3[1] * dr3[1] + dr3[2] * dr3[2])\n",
    "    PIPAngle = math.degrees(math.acos((dr2[0] * dr3[0] + dr2[1] * dr3[1] + dr2[2] * dr3[2])/(m1*m2)))\n",
    "    \n",
    "    m1 = m2\n",
    "    m2 = math.sqrt(dr4[0] * dr4[0] + dr4[1] * dr4[1] + dr4[2] * dr4[2])\n",
    "    MCPAngle = math.degrees(math.acos((dr3[0] * dr4[0] + dr3[1] * dr4[1] + dr3[2] * dr4[2])/(m1*m2)))\n",
    "    \n",
    "    for finger in onto.Finger.instances():\n",
    "        if a == 1 and finger.name == fingerDict[1]:\n",
    "            for i in finger.hasJoints:\n",
    "                if i.name == \"IP\":\n",
    "                    i.hasFlexionAngle = DIPAngle\n",
    "                elif i.name == \"MCP\":\n",
    "                    i.hasFlexionAngle = PIPAngle\n",
    "                elif i.name == \"CMC\":\n",
    "                    i.hasFlexionAngle = MCPAngle\n",
    "        elif finger.name == fingerDict[a]:\n",
    "            for i in finger.hasJoints:\n",
    "                if i.name == \"DIP\":\n",
    "                    i.hasFlexionAngle = DIPAngle\n",
    "                elif i.name == \"PIP\":\n",
    "                    i.hasFlexionAngle = PIPAngle\n",
    "                elif i.name == \"MCP\":\n",
    "                    i.hasFlexionAngle = MCPAngle\n",
    "    \n",
    "    return jointAngle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b49c1635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soumen/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/soumen/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector = HandDetector(maxHands=1, detectionCon=0.4)\n",
    "touchModel = models.AttnVGG(num_classes=160, normalize_attn=True)\n",
    "touchModel.load_state_dict(torch.load(\"/home/soumen/Soumen/2nd/Touch model/touch_model_final.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6da81b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populateTouch(image, key):\n",
    "    transform = transforms.Compose([dt.Rescale((227,227)), dt.ToTensor()])\n",
    "    #print(image.type)\n",
    "    sample = {'image':image.astype('float'), 'label':np.zeros([160]), 'name': key}\n",
    "    res = dt.Rescale((227,227))\n",
    "    trans = dt.ToTensor()\n",
    "    sample = res(sample)\n",
    "    sample = trans(sample)\n",
    "    image = sample['image']\n",
    "    #image = image.to(device)\n",
    "    #print(image.shape[0])\n",
    "    image = image.reshape([1, image.shape[0], image.shape[1], image.shape[2]])\n",
    "    #model = touchModel.to(device)\n",
    "    #print(image.shape)\n",
    "    out,_,_ = touchModel(image)\n",
    "    \n",
    "    print(out)\n",
    "    for i in range(len(out)):\n",
    "        if out[i].round() > 0:\n",
    "            print(key, i)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2030d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10172 13908\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(annotationPath):\n",
    "    os.mkdir(annotationPath)\n",
    "m = 0\n",
    "n = 0\n",
    "tracker = handTracker()\n",
    "for key in data:\n",
    "    m += 1\n",
    "    image = data[key]\n",
    "    #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    #hands, img = detector.findHands(image)\n",
    "    image = tracker.handsFinder(image)\n",
    "    lmlist = tracker.positionFinder(image)\n",
    "    #print(len(lmlist))\n",
    "    if len(lmlist)>0:\n",
    "        n+=1\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    m += 1\n",
    "    if hands:\n",
    "        n += 1\n",
    "        hand = hands[0]\n",
    "        lmlist = hand['lmList']\n",
    "        \n",
    "        index_MCP = lmlist[5]\n",
    "        little_MCP = lmlist[17]\n",
    "        centre = [(index_MCP[0]+little_MCP[0])/2, (index_MCP[1]+little_MCP[1])/2, (index_MCP[2]+little_MCP[2])/2]\n",
    "        for lm in lmlist:\n",
    "            result.append([lm[0]-centre[0], lm[1]-centre[1], lm[2]-centre[2]])\n",
    "        \n",
    "        ontoFile = \"Annotation_\" + key\n",
    "        #Calculate joint angles for each finger joint\n",
    "        thumbA = compute_joint_angle(result,1,2,3,4,ontoFile)\n",
    "        indexA = compute_joint_angle(result,5,6,7,8,ontoFile)\n",
    "        middleA = compute_joint_angle(result,9,10,11,12,ontoFile)\n",
    "        ringA = compute_joint_angle(result,13,14,15,16,ontoFile)\n",
    "        littleA = compute_joint_angle(result,17,18,19,20,ontoFile)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Constructing the annotation based on created ontology for each image\n",
    "        fullPath = os.path.join(annotationPath, ontoFile)\n",
    "        onto.save(file = fullPath, format=\"rdfxml\")\n",
    "    \"\"\"\n",
    "print(n,m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "108e69c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dataset = dt.TouchDataset(root_dir='Mudra Dataset/Single hand', num_classes=160,\n",
    "                                  transform=transforms.Compose([dt.Rescale((227,227)), dt.ToTensor()]))\n",
    "loader = dt.data_loader(all_dataset, batch_size=16, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a046f66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finger\n",
      "Finger\n",
      "Finger\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Annotation/Annotation_Mrigashirsha_136'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5644/2056956598.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Annotation_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Annotation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0monto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ontology\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFinger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mo_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/owlready2/namespace.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, only_local, fileobj, reload, reload_if_newer, url, **args)\u001b[0m\n\u001b[1;32m    969\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreload\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_last_update_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0m_LOG_LEVEL\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"* Owlready2 *     ...loading ontology %s from %s...\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 971\u001b[0;31m           \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    972\u001b[0m           \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m     \u001b[0mnew_base_iri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_base\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_orig_base_iri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m           \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Annotation/Annotation_Mrigashirsha_136'"
     ]
    }
   ],
   "source": [
    "for i, sample in enumerate(loader):\n",
    "    image = sample['image']\n",
    "    name = sample['name']\n",
    "    out,_,_ = touchModel(image)\n",
    "    o = out[0].detach().numpy()\n",
    "    output = np.empty((16,160))\n",
    "    for i in range(len(out)):\n",
    "        o = out[i].detach().numpy()\n",
    "        output = np.append(output, np.array(o), axis=1)\n",
    "        #output = np.vstack((output, o))\n",
    "    output = np.delete(output, slice(0,160), axis=1)\n",
    "    #print(output.shape[0])\n",
    "    for i in range(output.shape[0]):\n",
    "        fname = name[i]\n",
    "        fname = fname.split('.')[0]\n",
    "        fname = fname.split('/')\n",
    "        x = 'Annotation_'+fname[-2]+'_'+fname[-1]\n",
    "        filename = os.path.join('Annotation', x)\n",
    "        onto = get_ontology(filename).load()\n",
    "        print(onto.Finger.name)\n",
    "        o_arr = output[i,:].round()\n",
    "        for j in range(len(x)):\n",
    "            if o_arr[0] == 1:\n",
    "                break\n",
    "            elif o_arr[j] == 1:\n",
    "                if j <= 64:\n",
    "                    x = j/16 + 1\n",
    "                    y = j%16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2cdd7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
